{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python39264bitconda167c6283cb4747cd8cbdd21bc91332c6",
   "display_name": "Python 3.9.2 64-bit (conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Creating the corpus\n",
    "* Extracting a text corpus from Wikipedia made of plain text sentences\n",
    "* selected so as to have a roughly balanced corpus in terms of training data (each target category should be associated with the same number of sentences). \n",
    "\n",
    "* Including 6 categories:\n",
    "architects, mathematicians, painters, politicians, singers and writers.\n",
    "\n",
    "***\n",
    "\n",
    "* SCRIPT INPUT:\n",
    "    \n",
    "    * a number k of persons per category\n",
    "\n",
    "    * a number n of sentences per person\n",
    "    (persons whose wikipedia description is too short to have n sentences, should be ignored). \n",
    "\n",
    "* SCRIPT OUTPUT:\n",
    "\n",
    "    * the text of the corresponding Wikipedia page\n",
    "    \n",
    "    * the corresponding data type (A or Z) and category\n",
    "    \n",
    "    * WikiData description\n",
    "    \n",
    "            ▶ Store these into a csv or a json file and save it on your hard drive"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Part 1\n",
    "Create a list of persons you want to work with. Those persons\n",
    "should fall into two main types: artists (A) and non artists (Z).\n",
    "Singers, writers and painters are of type A while architects, politicians and mathematicians of type Z. For each category, select 30\n",
    "persons of that category. So in total you should have a list of 180\n",
    "persons (half of them are artists and half of them are not).\n",
    "You can use the wikidata warehouse to find persons of the expected categories. More precisely, the Wikidata collection can be\n",
    "filtered out using the SPARQL language and the following endpoint: https://query.wikidata.org/.\n",
    "You can thus use the SPARQLwrapper python library to apply a\n",
    "SPARQL query to the Wikidata warehouse and retrieve the required item identifiers."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import wikipedia\n",
    "from random import Random\n",
    "from spacy.lang.en import English\n",
    "from itertools import islice\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store wiki list pages\n",
    "wiki_pages = {}\n",
    "wiki_pages['singer'] = wikipedia.page('List of singers')\n",
    "wiki_pages['writer'] = wikipedia.page('List of writers')\n",
    "wiki_pages['painter'] = wikipedia.page('List of painters')\n",
    "wiki_pages['architect'] = wikipedia.page('List of architects')\n",
    "wiki_pages['politician'] = wikipedia.page('List of politicians by nationality')\n",
    "wiki_pages['mathematician'] =  wikipedia.page('List of mathematicians')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_article_keyword(wiki_page):\n",
    "    #if article\n",
    "    \n",
    "\n",
    "    #if list\n",
    "    keyword = wiki_page.title #after list(s) of\n",
    "    for category in wiki_page.categories:\n",
    "        if keyword in category:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeywordArticleChecker:\n",
    "    def __init__(self,keyword):\n",
    "        self.keyword = keyword\n",
    "\n",
    "    def __call__(self,wiki_page):\n",
    "        for category in wiki_page.categories:\n",
    "            if self.keyword in category:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_list_keyword(wiki_page):\n",
    "    \n",
    "    if keyword in wiki_page.title:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_is_not_list(wiki_page):   \n",
    "    for category in wiki_page.categories:\n",
    "        if category.lower().startswith('list'):\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_articles_from_dic(wiki_page, \n",
    "                            n_articles,\n",
    "                            page_filter=lambda x: True,\n",
    "                            check_is_final_node=check_is_not_list,\n",
    "                            rng=Random(),\n",
    "                            max_depth = 10,\n",
    "                            _memory=None):\n",
    "\n",
    "    print('  Checking:', wiki_page.title)\n",
    "    # if the function is being called for the first time, assign an empty set \n",
    "    if _memory is None:\n",
    "        _memory = set()\n",
    "\n",
    "    # trivial cases:\n",
    "    # if the number of articles retrieved satisfies the request\n",
    "    # or the current wiki_page has already been visited\n",
    "    # return an empty list\n",
    "    if (wiki_page.title in _memory \n",
    "        or n_articles <= 0\n",
    "        or not page_filter(wiki_page)\n",
    "        or max_depth == 0):\n",
    "        return []\n",
    "\n",
    "    # if the page has not been visited yet, add it to the _memory\n",
    "    _memory.add(wiki_page.title)\n",
    "\n",
    "    # if the wiki_page is an article (not a list),\n",
    "    # return it \n",
    "    if check_is_final_node(wiki_page):\n",
    "        print('▶ Added to list:',wiki_page.title)\n",
    "        return [wiki_page]\n",
    "    \n",
    "    # else, the page is a list\n",
    "    else:\n",
    "        articles = []\n",
    "        # shuffle the links of the list\n",
    "        while n_articles > 0:\n",
    "            title = rng.choices(wiki_page.links, k=1)   \n",
    "            try:\n",
    "                page = wikipedia.page(title)\n",
    "            except wikipedia.exceptions.WikipediaException:\n",
    "                continue\n",
    "\n",
    "            # get articles from the list links and add them to the list of articles\n",
    "            local_articles = get_articles_from_dic(wiki_page=page, \n",
    "                                                    n_articles=1, \n",
    "                                                    page_filter=page_filter,\n",
    "                                                    check_is_final_node=check_is_final_node,\n",
    "                                                    rng=rng,\n",
    "                                                    max_depth = max_depth-1,\n",
    "                                                    _memory=_memory)\n",
    "            articles.extend(local_articles)\n",
    "\n",
    "            n_articles -= len(local_articles)\n",
    "\n",
    "            # if the number of articles retrieved satisfies the request\n",
    "            # return the articles\n",
    "            if n_articles == 0:\n",
    "                return articles\n",
    "            elif n_articles < 0:\n",
    "                return articles[:n_articles]\n",
    "\n",
    "        return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "s\n",
      "  Checking: List of French-language authors\n",
      "  Checking: List of Belarusian writers\n",
      "  Checking: List of Emirati writers\n",
      "  Checking: Tobias S. Buckell\n",
      "  Checking: List of Salvadoran writers\n",
      "  Checking: List of German-language authors\n",
      "  Checking: List of Guinean writers\n",
      "  Checking: List of Colombian writers\n",
      "  Checking: List of Macedonian writers\n",
      "  Checking: List of Russian-language writers\n",
      "  Checking: List of Peruvian writers\n",
      "  Checking: List of Irish writers\n",
      "  Checking: Gus John\n",
      "  Checking: List of Uruguayan writers\n",
      "  Checking: List of Guinean writers\n",
      "  Checking: List of Ghanaian writers\n",
      "  Checking: Alister Hughes\n",
      "  Checking: List of Austrian writers\n",
      "  Checking: List of Trinidad and Tobago writers\n",
      "  Checking: List of Kenyan writers\n",
      "  Checking: List of Russian-language writers\n",
      "  Checking: List of Azerbaijani writers\n",
      "  Checking: List of Emirati writers\n",
      "  Checking: List of Tanzanian writers\n",
      "  Checking: List of Sri Lankan writers\n",
      "  Checking: List of Chinese writers\n",
      "  Checking: Don Rojas\n",
      "  Checking: List of Spanish writers\n",
      "  Checking: List of Barbadian writers\n",
      "  Checking: List of Somali writers\n",
      "  Checking: List of Peruvian writers\n",
      "  Checking: List of Barbadian writers\n",
      "  Checking: Joan Anim-Addo\n",
      "  Checking: List of Venezuelan writers\n",
      "  Checking: List of Moroccan writers\n",
      "  Checking: List of Nepali writers\n",
      "  Checking: List of Iranian writers\n",
      "  Checking: List of Azerbaijani writers\n",
      "  Checking: List of Moroccan writers\n",
      "  Checking: List of Chinese writers\n",
      "  Checking: List of Spanish writers\n",
      "  Checking: List of Russian-language writers\n",
      "  Checking: Don Rojas\n",
      "  Checking: List of Venezuelan writers\n",
      "  Checking: Grenada\n",
      "  Checking: List of Albanian writers\n",
      "  Checking: List of Democratic Republic of the Congo writers\n",
      "  Checking: List of Azerbaijani writers\n",
      "  Checking: List of Canadian writers\n",
      "  Checking: List of French-language authors\n",
      "  Checking: List of Russian-language writers\n",
      "  Checking: List of Greek writers\n",
      "  Checking: List of Cuban writers\n",
      "  Checking: List of Jamaican writers\n"
     ]
    }
   ],
   "source": [
    "wiki_keywords_people = {}\n",
    "for k,v in wiki_pages.items():\n",
    "    wiki_keywords_people[k] = get_articles_from_dic(wiki_page=v,\n",
    "                                        n_articles=30, \n",
    "                                        page_filter=KeywordArticleChecker(k),\n",
    "                                        rng=Random(0))\n",
    "wiki_keywords_people"
   ]
  },
  {
   "source": [
    "## Part 2\n",
    "for each selected person, retrieve his.her Wikidata description and\n",
    "Wikipedia page title. This can be done using the wikidata API\n",
    "along with the wptools python library.\n",
    "Once you have a list of wikipedia page titles, fetch (if it exists) the corresponding English wikipedia page, and extract the n first sentences of its content."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpacySentenceTokenizer:\n",
    "    def __init__(self, nlp=English()):\n",
    "        self.nlp = nlp\n",
    "        nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
    "\n",
    "    def __call__(self, txt):\n",
    "        return self.nlp(txt).sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_titles_descriptions_sentences(wiki_page : wikipedia.WikipediaPage,\n",
    "                                        n_sentences,\n",
    "                                        sentence_tokenize=SpacySentenceTokenizer()):\n",
    "    \n",
    "    title = wiki_page.title\n",
    "    description = wiki_page.summary # TODO: wikidata\n",
    "    content = wiki_page.content\n",
    "    sentences = [sent.string.strip() for sent in islice(sentence_tokenize(content), n_sentences)]\n",
    "    \n",
    "    return (title, description, sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = {}\n",
    "\n",
    "for k,v in wiki_keywords_people.items():\n",
    "    data[k] = []\n",
    "    for page in v:\n",
    "        article = {}\n",
    "        t,d,s = get_titles_descriptions_sentences(page, 10)\n",
    "        article['title'] = t\n",
    "        article['description'] = d\n",
    "        article['sentences'] = s\n",
    "        data[k].append(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['singer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split categories\n",
    "A = ['singer', 'writer', 'painter']\n",
    "Z = ['architect', 'politician', 'mathematician']\n",
    "\n",
    "A_cat = {a:data[a] for a in A}\n",
    "\n",
    "Z_cat = {z:data[z] for z in Z}\n",
    "\n",
    "data = {'A':A_cat, 'Z':Z_cat}"
   ]
  },
  {
   "source": [
    "## part 3\n",
    "Save data for preprocessing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.json', 'w') as f:\n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}