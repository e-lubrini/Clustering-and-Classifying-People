{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Lecture 13: Classification\n",
    "\n",
    "\n",
    "In this set of exercises, we will use classification to classify news articles into 5 topics. The dataset consists of 2225 documents and 5 categories: business, entertainment, politics, sport, and technology. \n",
    "\n",
    "\n",
    "The exercises cover the following points:\n",
    "\n",
    "* Storing the data into an pandas dataframe and inspecting the data\n",
    "* Converting the corpus into a tfd-idf document token matrix\n",
    "* Learning a perceptron model from the data \n",
    "* Inspecting the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the data\n",
    "\n",
    "#### Exercice 1.1 Create a pandas dataframe containing the news data\n",
    "\n",
    "* The data file is in \"data/bbc/\"\n",
    "* Use the [load_files](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_files.html) method from sklearn.datasets to load all files \n",
    "* load_files returns a dictionnary with keys \"data\" and \"labels\". \n",
    "* Use [Pandas DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) method to create a dataframe whose headers are \"texts\" and \"labels\" (the text is the data from load_files)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    " data_path = \"data/bbc/\" \n",
    "\n",
    "data = load_files(data_path, encoding=\"utf-8\", decode_error=\"replace\")  \n",
    "\n",
    "df = pd.DataFrame(list(zip(data['data'],data['target'])), columns=['texts', 'labels'])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Exercise 1.2  - Exploring the data and finding out which labels is associated with each domain\n",
    "\n",
    "* Print out the shape of the dataframe to find out how many BBC news report there are in the dataframe created in Exercise 1.1\n",
    "* Print out the dataframe to find out which label (an integer) is associated with which topic.\n",
    "* Define a dictionary tag_to_ix which maps the integers in the \"labels\" domain to the corresponding topic in the BBC data (these topics as: Business, Entertainment, Politics, Sport and Technology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the shape of the dataframe\n",
    "print(df.shape) \n",
    "\n",
    " \n",
    "\n",
    "# We print the different category names and labels. \n",
    "\n",
    "print(set(data['target'])) \n",
    "\n",
    "categories = pd.DataFrame(list(data['target_names']), columns=['names']) \n",
    "\n",
    "display(categories) \n",
    "\n",
    " \n",
    "\n",
    "# Print to see the articles associated to given label values. \n",
    "\n",
    "df[df['labels'] == 0].head() \n",
    "\n",
    "df[df['labels'] == 1].head() \n",
    "\n",
    "df[df['labels'] == 2].head() \n",
    "\n",
    "df[df['labels'] == 3].head() \n",
    "\n",
    "df[df['labels'] == 4].head() \n",
    "\n",
    " \n",
    "\n",
    "# The insert may only be done once per initialization! \n",
    "\n",
    "labels = [0,1,2,3,4] \n",
    "\n",
    "categories.insert(loc=1, column='labels', value=labels) \n",
    "\n",
    "display(categories) \n",
    "\n",
    " \n",
    "\n",
    "# If you want a merged version (similarly to databases' inner joins): \n",
    "\n",
    "merge = pd.merge(df ,categories ,on=['labels','labels']) \n",
    "\n",
    "merge.drop_duplicates(subset=[\"labels\"],keep=\"first\")[[\"texts\", \"names\"]].head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Exercise 1.3: Don't forget to shuffle the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle \n",
    "\n",
    "df= shuffle(df) \n",
    "\n",
    "df.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Vectorizing the input texts\n",
    "\n",
    "#### Exercise 2.2 \n",
    "\n",
    "* Extract $X$ and $Y$ from the dataframe\n",
    "* $Y$ = the category (business, etc.) of each BBC news item\n",
    "* $X$ = the features used for clustering. The features of a news items is the list of tokens contained in that item. We hope that words can help classify news items into the correct category: business, entertainment, politics, sport or  technology.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining X and Y\n",
    "Y = data[label] \n",
    "\n",
    "X = data['text'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.3 Create train and test data\n",
    "\n",
    "* Use sklearn train_test_split method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import 'train_test_split'\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Shuffle and split the data into training and testing subsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.4 Vectorizing the data\n",
    "\n",
    "Use sklearn [TfidfVectorizer]( https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) method to turn the news items into a TF-IDF matrix where each row represents a news item, the columns are tokens and the cell contains the tf-idf score of each token.\n",
    "\n",
    "* Import the TfidfVectorizer method from sklearn\n",
    "* Create a tf-idf vectorizer. The maximum nb of features should be set to 8000. Set use_idf to True, stop_words to \"english\" and the tokenizer to nltk.word_tokenize.\n",
    "* Apply the tfidf_vectorizer.fit_transform method to X to vectorize all input texts (i.e., both X_train and X_test)\n",
    "* Print out the shape of the training and test data\n",
    "* Print out the size of the vocabulary (use tfidf_vectorizer.vocabulary_ to extract the vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "\n",
    "from nltk import word_tokenize \n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "  \n",
    "\n",
    "# Using TFIDF vectorizer to convert convert words to Vector Space \n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=8000,  \n",
    "\n",
    "                                   use_idf=True,  \n",
    "\n",
    "                                   stop_words='english',  \n",
    "\n",
    "                                   tokenizer=nltk.word_tokenize, \n",
    "\n",
    "                                   ngram_range=(1, 3)) \n",
    "\n",
    "  \n",
    "\n",
    "# Fit the vectorizer to train and test data \n",
    "\n",
    "X_train_tf = tfidf_vectorizer.fit_transform(X_train) \n",
    "\n",
    "X_test_tf = tfidf_vectorizer.transform(X_test) \n",
    "\n",
    "  \n",
    "\n",
    "# Print the shape of the matrices X_train and X_test and the size of the vocabulary \n",
    "\n",
    "print(X_train_tf.shape) \n",
    "\n",
    "print(X_test_tf.shape) \n",
    "\n",
    "print( \"Vocabulary size:\", len(tfidf_vectorizer.vocabulary_) ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.5 - Use the get_feature_names method to print out the features\n",
    "\n",
    "This allows use to spot things that may not be right. E.g., oes the vocabulary contain uninformative tokens such as numbers, punctuation signs ? We won't work on this here (you have often done the linguistic preprocessing by now so it's not that useful as a learning exercise here) but it is good practice to look at your vocabulary before launching the machine learning to avoid learning on noisy data and henceforth get low results from your ML algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = tfidf_vectorizer.get_feature_names() \n",
    "\n",
    "print(features)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training a perceptron classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Exercise 3.1\n",
    "\n",
    "* Import the [Perceptron](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html) module \n",
    "* Create an object of the class Perceptron\n",
    "* Train the model using the [fit](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html#sklearn.linear_model.Perceptron.fit) method\n",
    "* Test the model using the [predict](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html#sklearn.linear_model.Perceptron.fit)\n",
    " method\n",
    "* Print out expected values and predictions\n",
    "* Print out accuracy using [sklearn accuracy_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron \n",
    "\n",
    "from sklearn.metrics import accuracy_score \n",
    "\n",
    "  \n",
    "\n",
    "# Create a Perceptron object \n",
    "\n",
    "clf = Perceptron(max_iter=5, tol=None) \n",
    "\n",
    "  \n",
    "\n",
    "# Train the model on the training data \n",
    "\n",
    "clf.fit(X_train_tf, Y_train) \n",
    "\n",
    "  \n",
    "\n",
    "# Test the model on the test data \n",
    "\n",
    "predictions = clf.predict(X_test_tf) \n",
    "\n",
    "  \n",
    "\n",
    "# Print out the expected values and the predictions \n",
    "\n",
    "print('Expected Values:', list(Y_test)) \n",
    "\n",
    "print('Predictions:', list(predictions)) \n",
    "\n",
    "\n",
    "# Print accuracy \n",
    "\n",
    "print( \"Accuracy:\", accuracy_score(Y_test, predictions)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.2\n",
    "\n",
    "* sklearn tfidf_vectorizer creates a vocabulary dictionary {(k,v),} where k is a token and v is an index (integer)\n",
    "   - Create a dictionary ix_to_tag mapping each index to the corresponding token  and a dictionary tag_to_idx mapping each token to the corresponding index\n",
    "* The [coef_ ](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html#sklearn.linear_model.Perceptron.fit) attribute contains the learned weights for each feature. Size = nb of classes, nb of features. \n",
    "* Save the feature weights in a dictionary where key = token index, value = weight\n",
    "* Define a function that derives a sorted list of (tokenIndex, weight) pairs\n",
    "* For each class, \n",
    "   -  get the feature weights for each class\n",
    "   - Sort the weights\n",
    "   - Print out the first 6 token:weight pairs (replace token indices by the corresponding token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token to index dictionary is already in the tfidf model \n",
    "\n",
    "token2idx = tfidf.vocabulary_ \n",
    "\n",
    "# inverse of the dictionary \n",
    "\n",
    "idx2token = {v: k for k, v in token2idx.items()} \n",
    "\n",
    "  \n",
    "\n",
    "# clf.coef_ yields matrix with classes as rows and tokens/features as columns \n",
    "\n",
    "# we don't have access to anything else, so we retrieve tokens weights from the inverse of this matrix \n",
    "\n",
    "idx2weight = {i: weight for i, weight in enumerate(clf.coef_.T)} \n",
    "\n",
    "weight : vector of size 5  \n",
    "\n",
    " \n",
    "\n",
    "# {0: array([-0.10224227,  0.34115365, -0.23020905, -0.15069405, -0.2137474 ]), \n",
    "1: array([-0.2794715 ,  0.        ,  0.06609557,  0.11862325,  0.06047473]), \n",
    "2: array([ 0.76770035,  0.20680919, -0.54074955, -0.35905334, -0.34783302]), \n",
    "3: array([ 0.60604639, -0.54384953, -0.22093773, -0.51137058, -0.39435784]), ... \n",
    "\n",
    " \n",
    "\n",
    "top_n = 6 \n",
    "\n",
    "  \n",
    "\n",
    "# argsort on the clf.coef_ sorts each row (axis=1) increasingly and yields indices instead of the actual values \n",
    "\n",
    "argsorted_cls = np.argsort(clf.coef_, axis=1) \n",
    "\n",
    "# argsorted_cls: matrix of size C X D (C: number of classes, D: number of features) \n",
    "\n",
    "  \n",
    "\n",
    "# we loop over the obtained, sorted indices, keeping the index number (representing the class index) \n",
    "\n",
    "for class_index, sorted_tokens in enumerate(argsorted_cls): \n",
    "\n",
    "    # using idx2target we can obtain classes actual name \n",
    "\n",
    "    print(f\"Class {idx2target[class_index]} ({class_index}) and it's top {top_n} tokens:\") \n",
    "\n",
    "     \n",
    "\n",
    "    # we need to inverse the obtained indices from the argsorted_cls, to make it decreasing \n",
    "\n",
    "    # we are interested in top 6 results \n",
    "\n",
    "    for token in sorted_tokens[::-1][:top_n]:  \n",
    "\n",
    "        # we can use idx2weight to obtain back the token's weight \n",
    "\n",
    "        # from this we can check and verify both: \n",
    "\n",
    "        #   1) tokens are really ranked from top 1 to top 6 \n",
    "\n",
    "        #   2) among classes, the highest value is being assigned to the class \n",
    "\n",
    "        #      to which the token has been located at as the top one \n",
    "\n",
    "        reformatted_weights = ', '.join([f\"{x:.4f}\" for x in idx2weight[token].tolist()]) \n",
    "\n",
    "        # idx2token allows us to obtain the token's actual name \n",
    "\n",
    "        print(f\"Token {idx2token[token]} ({token})  \n",
    "\n",
    "has a weight:\\n\\t[{reformatted_weights}]\") \n",
    "\n",
    "    print() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}