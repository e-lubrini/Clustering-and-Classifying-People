{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Creating the corpus\n",
    "* Extracting a text corpus from Wikipedia made of plain text sentences\n",
    "* selected so as to have a roughly balanced corpus in terms of training data (each target category should be associated with the same number of sentences). \n",
    "\n",
    "* Including 6 categories:\n",
    "architects, mathematicians, painters, politicians, singers and writers.\n",
    "\n",
    "***\n",
    "\n",
    "* SCRIPT INPUT:\n",
    "    \n",
    "    * a number k of persons per category\n",
    "\n",
    "    * a number n of sentences per person\n",
    "    (persons whose wikipedia description is too short to have n sentences, should be ignored). \n",
    "\n",
    "* SCRIPT OUTPUT:\n",
    "\n",
    "    * the text of the corresponding Wikipedia page\n",
    "    \n",
    "    * the corresponding data type (A or Z) and category\n",
    "    \n",
    "    * WikiData description\n",
    "    \n",
    "            â–¶ Store these into a csv or a json file and save it on your hard drive"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Part 1\n",
    "Create a list of persons you want to work with. Those persons\n",
    "should fall into two main types: artists (A) and non artists (Z).\n",
    "Singers, writers and painters are of type A while architects, politicians and mathematicians of type Z. For each category, select 30\n",
    "persons of that category. So in total you should have a list of 180\n",
    "persons (half of them are artists and half of them are not).\n",
    "You can use the wikidata warehouse to find persons of the expected categories. More precisely, the Wikidata collection can be\n",
    "filtered out using the SPARQL language and the following endpoint: https://query.wikidata.org/.\n",
    "You can thus use the SPARQLwrapper python library to apply a\n",
    "SPARQL query to the Wikidata warehouse and retrieve the required item identifiers."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "## Part 2\n",
    "for each selected person, retrieve his.her Wikidata description and\n",
    "Wikipedia page title. This can be done using the wikidata API\n",
    "along with the wptools python library."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "## Part 3\n",
    "Once you have a list of wikipedia page titles, fetch (if it exists)\n",
    "the corresponding English wikipedia page, and extract the n first\n",
    "sentences of its content."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}